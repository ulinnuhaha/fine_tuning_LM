{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:\\Ernst\\bi_direction Ita and Lad\\llama and gpt\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "# check current path\n",
    "current_path = os.getcwd()\n",
    "os.chdir(current_path)\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Convert the dataset into JSONL before performin fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CSV file into .jsonl file\n",
    "# We assume that the CSV file contains two columns (Source and target sentences)\n",
    "!python json_converter_tai.py \\\n",
    "  --dataset_dir ./dataset \\\n",
    "  --file_name eng2lad_dataset \\\n",
    "  --batch_sample 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FT model of Llama using Together AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together.utils import check_file\n",
    "file_name = 'dataset/eng2lad_dataset_tai.jsonl'\n",
    "report = check_file(file_name)\n",
    "print(report)\n",
    "assert report[\"is_check_passed\"] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Uploading & checking the dataset to Together AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "\n",
    "# Please set your API Key from Together AI\n",
    "client = Together(api_key='xxx')\n",
    "\n",
    "# Upload formatted data and get back the file ID\n",
    "response = client.files.upload(file=file_name)\n",
    "fileId = response.model_dump()[\"id\"]\n",
    "# Verify that the file was uploaded successfully\n",
    "file_metadata = client.files.retrieve(fileId)\n",
    "print(file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the uploaded file in Together AI\n",
    "import requests\n",
    "\n",
    "url = \"https://api.together.xyz/v1/files\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": \"Bearer xxxx\" # xxxx = api\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger fine-tuning job\n",
    "resp = client.fine_tuning.create(\n",
    "    suffix=\"mt_ita_lad\",\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Reference\",\n",
    "    training_file=fileId,\n",
    "    n_epochs=3,\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    lora = True,\n",
    "    lora_r = 32\n",
    "    #wandb_api_key=os.environ.get(\"WANDB_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Translation on the Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ft_test.py \\\n",
    "  --model_name ft_llama_31_70b \\\n",
    "  --dataset_dir ./dataset \\\n",
    "  --test_data test_3_eng2lad \\\n",
    "  --target_lang ladin \\\n",
    "  --batch_size 10 \\\n",
    "  --save_dir ./save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the translation results using evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import json\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "bleu_score = evaluate.load(\"bleu\")\n",
    "chrf_score = evaluate.load(\"chrf\")\n",
    "sacrebleu_score = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(predictions, references):\n",
    "    # Ensure both predictions and references are lists of strings\n",
    "    predictions = [str(pred) if pred is not None else \"\" for pred in predictions]\n",
    "    references = [str(ref) if ref is not None else \"\" for ref in references]\n",
    "\n",
    "    # Compute scores\n",
    "    result = rouge_score.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "    score = sacrebleu_score.compute(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )\n",
    "    result[\"sacrebleu\"] = score[\"score\"]\n",
    "    bleu = bleu_score.compute(predictions=predictions, references=references)\n",
    "    result[\"bleu\"] = bleu['bleu']\n",
    "    chrf = chrf_score.compute(predictions=predictions, references=references) ##The higher the value, the better the translations\n",
    "    chrf_plus = chrf_score.compute(predictions=predictions, references=references, word_order=2)  # chrF++\n",
    "    result[\"chrf++\"] = chrf_plus[\"score\"]\n",
    "    result[\"chrf\"] = chrf[\"score\"] #The higher the value, the better the translations\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the translation result with the gound truth\n",
    "def get_json_files(llm_model, test_data, target_lang, batch_size):\n",
    "    # Get JSON files for specific translation test data using a specific llm\n",
    "    # Define the file prefix file name\n",
    "    file_prefix = (f'translation_{llm_model}_{test_data}_eng2lad_size of_{batch_size}_batch_')\n",
    "  \n",
    "    # List all files in the directory that start with the specified prefix\n",
    "    save_dir = 'save_results'\n",
    "    matching_files = [f for f in os.listdir(save_dir) if f.startswith(file_prefix)] #current_path+'/save_results'\n",
    "    # Count the number of matching files\n",
    "    num_files = len(matching_files)\n",
    "    print(f\"Found {num_files} files.\")\n",
    "\n",
    "    scores = {}\n",
    "    scores['rouge1'] = []\n",
    "    scores['rouge2'] = []\n",
    "    scores['rougeL'] = []\n",
    "    scores['bleu'] = []\n",
    "    scores['chrf'] = []\n",
    "    scores['sacrebleu'] = []\n",
    "    scores['chrf++'] = []\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bleu': [], 'chrf': [], 'sacrebleu': [], 'chrf++': []}\n",
    "    all_scores = []\n",
    "    batch_start = 0\n",
    "    # get the ral data / ground truth\n",
    "    ref_data = pd.read_csv(f'dataset/{test_data}_ita2lad.csv')\n",
    "    for i in range(num_files):\n",
    "         # Slicing for the current batch of data\n",
    "        real_data = ref_data.iloc[batch_start:batch_start + batch_size]\n",
    "        batch_start = (i + 1) * batch_size\n",
    "        print(f\"Processing batch {i+1}, starting at index {batch_start}\")\n",
    "        # Get the real data as a list\n",
    "        real_data = real_data[target_lang].tolist() \n",
    "\n",
    "        # Open and read the JSON files of translation result\n",
    "        file_loc=os.path.join(save_dir+f'/{file_prefix}{i}.json') #save_dir\n",
    "        print(\"load the json file\", file_loc)\n",
    "        f = open(file_loc, encoding='utf8')\n",
    "        data = json.load(f)\n",
    "        # Get the target translation using llm API\n",
    "        # if json data is in str, convert to dict\n",
    "        if isinstance(data, str):\n",
    "            data = json.loads(data)\n",
    "\n",
    "        # Ensure 'choices' exists and contains data\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            translation_output = data[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            #print(translation_output)\n",
    "            if \"choices\" in data and data[\"choices\"]:\n",
    "                translation_output = data[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "                if translation_output.strip():  # Check if content is not empty\n",
    "                    #if isinstance(translation_output, str):\n",
    "                    # Clean and process translation output\n",
    "                    translation_output = translation_output.strip('```json\\n').strip('```')\n",
    "                     \n",
    "                    # Remove the additional response\n",
    "                    if translation_output.startswith(f\"Here are the {target_lang} translations:\"):\n",
    "                        translation_output = translation_output.replace(f\"Here are the {target_lang} translations:\", \"\").strip()\n",
    "                    \n",
    "                    # Convert the remaining text to a Python list\n",
    "                    try:\n",
    "                        \n",
    "                        ladin_translations = eval(translation_output)  # Caution: Use `eval` only if you're sure of the data source\n",
    "                        \n",
    "                        # calculate the evaluation metric score\n",
    "                        \n",
    "                        scores =eval_metrics(ladin_translations, real_data)\n",
    "                        all_scores.append(\n",
    "                                {'rouge1': scores['rouge1'],\n",
    "                                'rouge2':scores['rouge2'],\n",
    "                                'rougeL': scores['rougeL'],\n",
    "                                'bleu': scores['bleu'],\n",
    "                                'sacrebleu': scores['sacrebleu'],\n",
    "                                'chrf': scores['chrf'],\n",
    "                                'chrf++': scores['chrf++']}\n",
    "                                )\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(translation_output)\n",
    "                        print(f\"Error parsing the translations: {e}\")\n",
    "            else:\n",
    "                print(\"No translations found.\")\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter for performing Evaluations\n",
    "test_data = 'test_1'# test_2 / test_3\n",
    "llm_model = 'ft_llama_31_70b_tai' #\n",
    "batch_size = 15\n",
    "target_lang = 'ladin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_result=get_json_files(llm_model, test_data, target_lang, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "fr = pd.DataFrame(translation_result)\n",
    "print(len(translation_result))\n",
    "# Calculate the mean for each column\n",
    "mean_scores = fr.mean()\n",
    "# Print the mean scores\n",
    "print(mean_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
